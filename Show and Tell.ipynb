{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show and Tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the simplest model based on a [Show and Tell paper](https://arxiv.org/pdf/1411.4555.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = 'datasets'\n",
    "DATASET = 'mini_coco'\n",
    "ANNOTATIONS_PATH = 'annotations/captions_{0}2014.json'\n",
    "IMAGES_PATH = 'images/{0}2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "\n",
    "train_dataset = torchvision.datasets.CocoCaptions(\n",
    "    root = os.path.join(ROOT, DATASET, IMAGES_PATH.format('train')),\n",
    "    annFile = os.path.join(ROOT, DATASET, ANNOTATIONS_PATH.format('train')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = defaultdict(int)\n",
    "\n",
    "for image, texts in train_dataset:\n",
    "    for text in texts:\n",
    "        text = clean_text(text)\n",
    "        for word in text:\n",
    "            c[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_filtered = [word for word in c if c[word] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<START>'\n",
    "UNK = '<UNK>'\n",
    "END = '<END>'\n",
    "\n",
    "c_filtered.append(START)\n",
    "c_filtered.append(UNK)\n",
    "c_filtered.append(END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2w = {}\n",
    "w2i = {}\n",
    "\n",
    "for index, word in enumerate(c_filtered):\n",
    "    i2w[index] = word\n",
    "    w2i[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    sequence = [w2i[START]]\n",
    "    for word in text:\n",
    "        if word in w2i:\n",
    "            sequence.append(w2i[word])\n",
    "        else:\n",
    "            sequence.append(w2i[UNK])\n",
    "    sequence.append(w2i[END])\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Resize((200, 200)),\n",
    "     torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(batch):\n",
    "    images_list = []\n",
    "    texts_list = []\n",
    "    for image, texts in batch:\n",
    "        image = transform(image)\n",
    "        images_list += [image] * len(texts)\n",
    "        \n",
    "        for text in texts:\n",
    "            texts_list.append(torch.tensor(transform_text(text)))\n",
    "            \n",
    "    images_list, texts_list = \\\n",
    "        list(zip(*sorted(zip(images_list, texts_list), key=lambda x: x[1].shape[0], reverse=True)))\n",
    "    \n",
    "    inputs = [text[:-1] for text in texts_list]\n",
    "    outputs = [text[1:] for text in texts_list]\n",
    "    \n",
    "    packed_inputs = torch.nn.utils.rnn.pack_sequence(inputs, enforce_sorted=True)\n",
    "    packed_outputs = torch.nn.utils.rnn.pack_sequence(outputs, enforce_sorted=True)\n",
    "    return torch.stack(images_list), packed_inputs, packed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_validate(batch):\n",
    "    images_list = []\n",
    "    texts_list = []\n",
    "    for image, texts in batch:\n",
    "        images_list.append(transform(image))\n",
    "        texts = list(map(lambda text: ' '.join(clean_text(text)), texts))\n",
    "        texts_list.append(texts)\n",
    "    return torch.stack(images_list), texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_beam_search(model, image, w2i, i2w, max_length=15, beam_size=1):\n",
    "    # Here are two problems\n",
    "    # 1. Size of new_hyps on every iteration is beam_size ** 2,\n",
    "    # while we can use only 2 * beam_size memory\n",
    "    # 2. Here are some cycles that can be replaced with a numpy vectorized operations\n",
    "    #image = transform(image)\n",
    "    #image = torch.unsqueeze(image, 0)\n",
    "    \n",
    "    cur_hyps = [[w2i[START]]]\n",
    "    cur_probs = [1.]\n",
    "    cur_hiddens = model.encoder(image)\n",
    "    cur_hiddens = torch.unsqueeze(cur_hiddens, 0)\n",
    "    for i in range(max_length):\n",
    "        print('Ith beam: ', i)\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_sequence(\n",
    "            [torch.tensor([hyp[-1]]) for hyp in cur_hyps], enforce_sorted=True)\n",
    "        probs, hiddens = model.decoder(cur_hiddens, packed_inputs)\n",
    "        print(probs)\n",
    "        new_hyps = []\n",
    "        new_probs = []\n",
    "        new_hiddens = []\n",
    "        for hyp, cur_prob, prob, hidden in zip(cur_hyps, cur_probs, probs.data, hiddens.data.tolist()[0]):\n",
    "            if hyp[-1] == w2i[END]:\n",
    "                new_hyps.append(hyp)\n",
    "                new_probs.append(cur_prob)\n",
    "                new_hiddens.append(hidden)\n",
    "                continue\n",
    "            max_words = torch.argsort(prob)[-beam_size:]\n",
    "            for word in max_words:\n",
    "                new_hyp = hyp.copy()\n",
    "                new_hyp.append(word.item())\n",
    "                new_hyps.append(new_hyp)\n",
    "                new_probs.append(cur_prob * prob[word].item())\n",
    "                new_hiddens.append(hidden)\n",
    "        new_probs = np.array(new_probs)\n",
    "        new_hiddens = torch.tensor(new_hiddens)\n",
    "        best_hyps = np.argsort(new_probs)[-beam_size:]\n",
    "        cur_probs = new_probs[best_hyps]\n",
    "        cur_hiddens = new_hiddens[best_hyps]\n",
    "        cur_hiddens = torch.unsqueeze(cur_hiddens, 0)\n",
    "        cur_hyps = []\n",
    "        for hyp_num in best_hyps:\n",
    "            cur_hyps.append(new_hyps[hyp_num])\n",
    "            \n",
    "    assert(np.argmax(cur_probs) == len(cur_probs) - 1)\n",
    "            \n",
    "    return cur_hyps[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# bleu_scorer.py\n",
    "# David Chiang <chiang@isi.edu>\n",
    "\n",
    "# Copyright (c) 2004-2006 University of Maryland. All rights\n",
    "# reserved. Do not redistribute without permission from the\n",
    "# author. Not for commercial use.\n",
    "\n",
    "# Modified by:\n",
    "# Hao Fang <hfang@uw.edu>\n",
    "# Tsung-Yi Lin <tl483@cornell.edu>\n",
    "\n",
    "''' Provides:\n",
    "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
    "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
    "'''\n",
    "\n",
    "import copy\n",
    "import sys, math, re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def precook(s, n=4, out=False):\n",
    "    \"\"\"Takes a string as input and returns an object that can be given to\n",
    "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
    "    can take string arguments as well.\"\"\"\n",
    "    words = s.split()\n",
    "    counts = defaultdict(int)\n",
    "    for k in range(1, n + 1):\n",
    "        for i in range(len(words) - k + 1):\n",
    "            ngram = tuple(words[i:i + k])\n",
    "            counts[ngram] += 1\n",
    "    return (len(words), counts)\n",
    "\n",
    "\n",
    "def cook_refs(refs, eff=None, n=4):  ## lhuang: oracle will call with \"average\"\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.'''\n",
    "\n",
    "    reflen = []\n",
    "    maxcounts = {}\n",
    "    for ref in refs:\n",
    "        rl, counts = precook(ref, n)\n",
    "        reflen.append(rl)\n",
    "        for (ngram, count) in counts.items():\n",
    "            maxcounts[ngram] = max(maxcounts.get(ngram, 0), count)\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "    if eff == \"shortest\":\n",
    "        reflen = min(reflen)\n",
    "    elif eff == \"average\":\n",
    "        reflen = float(sum(reflen)) / len(reflen)\n",
    "\n",
    "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
    "\n",
    "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
    "\n",
    "    return (reflen, maxcounts)\n",
    "\n",
    "\n",
    "def cook_test(test, ref_tuple, eff=None, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.'''\n",
    "\n",
    "    testlen, counts = precook(test, n, True)\n",
    "    reflen, refmaxcounts = ref_tuple\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "\n",
    "    if eff == \"closest\":\n",
    "        result[\"reflen\"] = min((abs(l - testlen), l) for l in reflen)[1]\n",
    "    else:  ## i.e., \"average\" or \"shortest\" or None\n",
    "        result[\"reflen\"] = reflen\n",
    "\n",
    "    result[\"testlen\"] = testlen\n",
    "\n",
    "    result[\"guess\"] = [max(0, testlen - k + 1) for k in range(1, n + 1)]\n",
    "\n",
    "    result['correct'] = [0] * n\n",
    "    for (ngram, count) in counts.items():\n",
    "        result[\"correct\"][len(ngram) - 1] += min(refmaxcounts.get(ngram, 0), count)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class BleuScorer(object):\n",
    "    \"\"\"Bleu scorer.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
    "\n",
    "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
    "\n",
    "    def copy(self):\n",
    "        ''' copy the refs.'''\n",
    "        new = BleuScorer(n=self.n)\n",
    "        new.ctest = copy.copy(self.ctest)\n",
    "        new.crefs = copy.copy(self.crefs)\n",
    "        new._score = None\n",
    "        return new\n",
    "\n",
    "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
    "        ''' singular instance '''\n",
    "\n",
    "        self.n = n\n",
    "        self.crefs = []\n",
    "        self.ctest = []\n",
    "        self.cook_append(test, refs)\n",
    "        self.special_reflen = special_reflen\n",
    "\n",
    "    def cook_append(self, test, refs):\n",
    "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
    "\n",
    "        if refs is not None:\n",
    "            self.crefs.append(cook_refs(refs))\n",
    "            if test is not None:\n",
    "                cooked_test = cook_test(test, self.crefs[-1])\n",
    "                self.ctest.append(cooked_test)  ## N.B.: -1\n",
    "            else:\n",
    "                self.ctest.append(None)  # lens of crefs and ctest have to match\n",
    "\n",
    "        self._score = None  ## need to recompute\n",
    "\n",
    "    def ratio(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._ratio\n",
    "\n",
    "    def score_ratio(self, option=None):\n",
    "        '''\n",
    "        return (bleu, len_ratio) pair\n",
    "        '''\n",
    "\n",
    "        return self.fscore(option=option), self.ratio(option=option)\n",
    "\n",
    "    def score_ratio_str(self, option=None):\n",
    "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
    "\n",
    "    def reflen(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._reflen\n",
    "\n",
    "    def testlen(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._testlen\n",
    "\n",
    "    def retest(self, new_test):\n",
    "        if type(new_test) is str:\n",
    "            new_test = [new_test]\n",
    "        assert len(new_test) == len(self.crefs), new_test\n",
    "        self.ctest = []\n",
    "        for t, rs in zip(new_test, self.crefs):\n",
    "            self.ctest.append(cook_test(t, rs))\n",
    "        self._score = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rescore(self, new_test):\n",
    "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
    "\n",
    "        return self.retest(new_test).compute_score()\n",
    "\n",
    "    def size(self):\n",
    "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
    "        return len(self.crefs)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''add an instance (e.g., from another sentence).'''\n",
    "\n",
    "        if type(other) is tuple:\n",
    "            ## avoid creating new BleuScorer instances\n",
    "            self.cook_append(other[0], other[1])\n",
    "        else:\n",
    "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
    "            self.ctest.extend(other.ctest)\n",
    "            self.crefs.extend(other.crefs)\n",
    "            self._score = None  ## need to recompute\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compatible(self, other):\n",
    "        return isinstance(other, BleuScorer) and self.n == other.n\n",
    "\n",
    "    def single_reflen(self, option=\"average\"):\n",
    "        return self._single_reflen(self.crefs[0][0], option)\n",
    "\n",
    "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
    "\n",
    "        if option == \"shortest\":\n",
    "            reflen = min(reflens)\n",
    "        elif option == \"average\":\n",
    "            reflen = float(sum(reflens)) / len(reflens)\n",
    "        elif option == \"closest\":\n",
    "            reflen = min((abs(l - testlen), l) for l in reflens)[1]\n",
    "        else:\n",
    "            assert False, \"unsupported reflen option %s\" % option\n",
    "\n",
    "        return reflen\n",
    "\n",
    "    def recompute_score(self, option=None, verbose=0):\n",
    "        self._score = None\n",
    "        return self.compute_score(option, verbose)\n",
    "\n",
    "    def compute_score(self, option=None, verbose=0):\n",
    "        n = self.n\n",
    "        small = 1e-9\n",
    "        tiny = 1e-15  ## so that if guess is 0 still return 0\n",
    "        bleu_list = [[] for _ in range(n)]\n",
    "\n",
    "        if self._score is not None:\n",
    "            return self._score\n",
    "\n",
    "        if option is None:\n",
    "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
    "\n",
    "        self._testlen = 0\n",
    "        self._reflen = 0\n",
    "        totalcomps = {'testlen': 0, 'reflen': 0, 'guess': [0] * n, 'correct': [0] * n}\n",
    "\n",
    "        # for each sentence\n",
    "        for comps in self.ctest:\n",
    "            testlen = comps['testlen']\n",
    "            self._testlen += testlen\n",
    "\n",
    "            if self.special_reflen is None:  ## need computation\n",
    "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
    "            else:\n",
    "                reflen = self.special_reflen\n",
    "\n",
    "            self._reflen += reflen\n",
    "\n",
    "            for key in ['guess', 'correct']:\n",
    "                for k in range(n):\n",
    "                    totalcomps[key][k] += comps[key][k]\n",
    "\n",
    "            # append per image bleu score\n",
    "            bleu = 1.\n",
    "            for k in range(n):\n",
    "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
    "                        / (float(comps['guess'][k]) + small)\n",
    "                bleu_list[k].append(bleu ** (1. / (k + 1)))\n",
    "            ratio = (testlen + tiny) / (reflen + small)  ## N.B.: avoid zero division\n",
    "            if ratio < 1:\n",
    "                for k in range(n):\n",
    "                    bleu_list[k][-1] *= math.exp(1 - 1 / ratio)\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(comps, reflen)\n",
    "\n",
    "        totalcomps['reflen'] = self._reflen\n",
    "        totalcomps['testlen'] = self._testlen\n",
    "\n",
    "        bleus = []\n",
    "        bleu = 1.\n",
    "        for k in range(n):\n",
    "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
    "                    / (totalcomps['guess'][k] + small)\n",
    "            bleus.append(bleu ** (1. / (k + 1)))\n",
    "        ratio = (self._testlen + tiny) / (self._reflen + small)  ## N.B.: avoid zero division\n",
    "        if ratio < 1:\n",
    "            for k in range(n):\n",
    "                bleus[k] *= math.exp(1 - 1 / ratio)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(totalcomps)\n",
    "            print(\"ratio:\", ratio)\n",
    "\n",
    "        self._score = bleus\n",
    "        return self._score, bleu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# File Name : bleu.py\n",
    "#\n",
    "# Description : Wrapper for BLEU scorer.\n",
    "#\n",
    "# Creation Date : 06-01-2015\n",
    "# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n",
    "# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n",
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        # default compute Blue score up to 4\n",
    "        self._n = n\n",
    "        self._hypo_for_image = {}\n",
    "        self.ref_for_image = {}\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        bleu_scorer = BleuScorer(n=self._n)\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) >= 1)\n",
    "\n",
    "            bleu_scorer += (hypo[0], ref)\n",
    "\n",
    "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
    "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
    "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
    "\n",
    "        # return (bleu, bleu_info)\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"Bleu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, w2i, i2w):\n",
    "    bleu = Bleu(n=4)\n",
    "    \n",
    "    gts_dict = {}\n",
    "    hyps_dict = {}\n",
    "    for i, (image, texts) in enumerate(dataloader):\n",
    "        hyp = simple_beam_search(model, image, w2i, i2w, beam_size=3)[1:]\n",
    "        if hyp[-1] == w2i['<END>']:\n",
    "            hyp = hyp[:-1]\n",
    "        hyp = ' '.join([i2w[word] for word in hyp])\n",
    "        gts_dict[i] = texts[0]\n",
    "        hyps_dict[i] = [hyp]\n",
    "    (bleu1, bleu2, bleu3, bleu4), _ = bleu.compute_score(gts_dict, hyps_dict)\n",
    "    return bleu1, bleu2, bleu3, bleu4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, dict_size, embedding_dim, hidden_size, *args, **kwargs):\n",
    "        super(SimpleModel, self).__init__(*args, **kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(in_features=10580, out_features=hidden_size)\n",
    "        self.encoder_layers = [\n",
    "            self.conv1, self.pooling, self.relu,\n",
    "            self.conv2, self.pooling, self.relu,\n",
    "            self.conv3, self.pooling, self.relu]\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=dict_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=dict_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def encoder(self, image):\n",
    "        for layer in self.encoder_layers:\n",
    "            image = layer(image)\n",
    "        return self.linear1(image.view(-1, 10580)).view(-1, self.hidden_size)\n",
    "    \n",
    "    def decoder(self, image_vector, input_captions):\n",
    "        embeddings = nn.utils.rnn.PackedSequence(\n",
    "            self.embedding(input_captions.data),\n",
    "            input_captions.batch_sizes)\n",
    "        decoded, hiddens = self.rnn(embeddings, image_vector)\n",
    "        probs = self.softmax(self.linear2(decoded.data))\n",
    "        return nn.utils.rnn.PackedSequence(probs, decoded.batch_sizes), hiddens\n",
    "\n",
    "    def forward(self, image, input_captions):\n",
    "        image_vector = self.encoder(image)\n",
    "        image_vector = image_vector.unsqueeze(0)\n",
    "        return self.decoder(image_vector, input_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(dict_size=len(w2i), embedding_dim=32, hidden_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    total_loss = 0.0\n",
    "    for image, inputs, outputs in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ans, _ = model(image, inputs)\n",
    "        loss = criterion(ans.data, outputs.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    bleu1, bleu2, bleu3, bleu4 = validate(model, valloader, w2i, i2w)\n",
    "\n",
    "    writer.add_scalar('loss', total_loss, epoch)\n",
    "    writer.add_scalar('bleu1', bleu1, epoch)\n",
    "    writer.add_scalar('bleu2', bleu2, epoch)\n",
    "    writer.add_scalar('bleu3', bleu3, epoch)\n",
    "    writer.add_scalar('bleu4', bleu4, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
