{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnYT4FjhDuIj"
   },
   "source": [
    "# Show and Tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6FbmT_4DuIl"
   },
   "source": [
    "Implementing the simplest model based on a [Show and Tell paper](https://arxiv.org/pdf/1411.4555.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBX7jHFCFfv-"
   },
   "source": [
    "## Prepare notebook if running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItRo8jIpENif"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlYIOrIYFkzS"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfLBzU2VGc79"
   },
   "source": [
    "## Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GaZGAAUGiGv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os-X3k-oDuIl"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw27yYhmDuIn"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    ROOT = 'drive/My Drive/test_rclone'\n",
    "else:\n",
    "    ROOT = 'datasets'\n",
    "\n",
    "DATASET = 'mini_coco'\n",
    "ANNOTATIONS_PATH = 'annotations/captions_{0}2014.json'\n",
    "IMAGES_PATH = 'images/{0}2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8aG0KtIDuIt"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "\n",
    "train_dataset = torchvision.datasets.CocoCaptions(\n",
    "    root = os.path.join(ROOT, DATASET, IMAGES_PATH.format('train')),\n",
    "    annFile = os.path.join(ROOT, DATASET, ANNOTATIONS_PATH.format('train')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlAUKIZCDuIy"
   },
   "source": [
    "## Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joMcefRgDuIz"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N649eIruHidP"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjuKlg2yDuI4"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJFYmHbsDuI_"
   },
   "outputs": [],
   "source": [
    "c = defaultdict(int)\n",
    "\n",
    "for image, texts in train_dataset:\n",
    "    for text in texts:\n",
    "        text = clean_text(text)\n",
    "        for word in text:\n",
    "            c[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QMUEWoADuJE"
   },
   "outputs": [],
   "source": [
    "c_filtered = [word for word in c if c[word] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jO6-LoR2DuJI"
   },
   "outputs": [],
   "source": [
    "START = '<START>'\n",
    "UNK = '<UNK>'\n",
    "END = '<END>'\n",
    "\n",
    "c_filtered.append(START)\n",
    "c_filtered.append(UNK)\n",
    "c_filtered.append(END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-DTxo2sDuJN"
   },
   "outputs": [],
   "source": [
    "i2w = {}\n",
    "w2i = {}\n",
    "\n",
    "for index, word in enumerate(c_filtered):\n",
    "    i2w[index] = word\n",
    "    w2i[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-DjhbfsDuJR"
   },
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    sequence = [w2i[START]]\n",
    "    for word in text:\n",
    "        if word in w2i:\n",
    "            sequence.append(w2i[word])\n",
    "        else:\n",
    "            sequence.append(w2i[UNK])\n",
    "    sequence.append(w2i[END])\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuRD0dFXDuJV"
   },
   "outputs": [],
   "source": [
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lFEK_HnDuJY"
   },
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zptANtzzDuJY"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9EhKYgODuJd"
   },
   "outputs": [],
   "source": [
    "def collate_fn_train(batch):\n",
    "    images_list = []\n",
    "    texts_list = []\n",
    "    for image, texts in batch:\n",
    "        image = transform(image)\n",
    "        images_list += [image] * len(texts)\n",
    "        \n",
    "        for text in texts:\n",
    "            texts_list.append(torch.tensor(transform_text(text)))\n",
    "            \n",
    "    images_list, texts_list = \\\n",
    "        list(zip(*sorted(zip(images_list, texts_list), key=lambda x: x[1].shape[0], reverse=True)))\n",
    "    \n",
    "    inputs = [text[:-1] for text in texts_list]\n",
    "    outputs = [text[1:] for text in texts_list]\n",
    "    \n",
    "    packed_inputs = torch.nn.utils.rnn.pack_sequence(inputs, enforce_sorted=True)\n",
    "    packed_outputs = torch.nn.utils.rnn.pack_sequence(outputs, enforce_sorted=True)\n",
    "    return torch.stack(images_list), packed_inputs, packed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twoJED3XDuJk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83guufIlDuJn"
   },
   "outputs": [],
   "source": [
    "def collate_fn_validate(batch):\n",
    "    images_list = []\n",
    "    texts_list = []\n",
    "    for image, texts in batch:\n",
    "        images_list.append(transform(image))\n",
    "        texts = list(map(lambda text: ' '.join(clean_text(text)), texts))\n",
    "        texts_list.append(texts)\n",
    "    return torch.stack(images_list), texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tgX9fZVDuJr"
   },
   "outputs": [],
   "source": [
    "valloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmtKW1XwDuJu"
   },
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atgfNW99DuJu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_beam_search(model, image, w2i, i2w, max_length=15, beam_size=1):\n",
    "    # Here are two problems\n",
    "    # 1. Size of new_hyps on every iteration is beam_size ** 2,\n",
    "    # while we can use only 2 * beam_size memory\n",
    "    # 2. Here are some cycles that can be replaced with a numpy vectorized operations\n",
    "    #image = transform(image)\n",
    "    #image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    cur_hyps = [[w2i[START]]]\n",
    "    cur_probs = [1.]\n",
    "    cur_hiddens = model.encoder(image)\n",
    "    cur_hiddens = torch.unsqueeze(cur_hiddens, 0)\n",
    "    for i in range(max_length):\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_sequence(\n",
    "            [torch.tensor([hyp[-1]]) for hyp in cur_hyps], enforce_sorted=True)\n",
    "        cur_hiddens = cur_hiddens.to(device)\n",
    "        packed_inputs = packed_inputs.to(device)\n",
    "        probs, hiddens = model.decoder(cur_hiddens, packed_inputs)\n",
    "        new_hyps = []\n",
    "        new_probs = []\n",
    "        new_hiddens = []\n",
    "        for hyp, cur_prob, prob, hidden in zip(cur_hyps, cur_probs, probs.data, hiddens.data.tolist()[0]):\n",
    "            if hyp[-1] == w2i[END]:\n",
    "                new_hyps.append(hyp)\n",
    "                new_probs.append(cur_prob)\n",
    "                new_hiddens.append(hidden)\n",
    "                continue\n",
    "            max_words = torch.argsort(prob)[-beam_size:]\n",
    "            for word in max_words:\n",
    "                new_hyp = hyp.copy()\n",
    "                new_hyp.append(word.item())\n",
    "                new_hyps.append(new_hyp)\n",
    "                new_probs.append(cur_prob * prob[word].item())\n",
    "                new_hiddens.append(hidden)\n",
    "        new_probs = np.array(new_probs)\n",
    "        new_hiddens = torch.tensor(new_hiddens)\n",
    "        best_hyps = np.argsort(new_probs)[-beam_size:]\n",
    "        cur_probs = new_probs[best_hyps]\n",
    "        cur_hiddens = new_hiddens[best_hyps]\n",
    "        cur_hiddens = torch.unsqueeze(cur_hiddens, 0)\n",
    "        cur_hyps = []\n",
    "        for hyp_num in best_hyps:\n",
    "            cur_hyps.append(new_hyps[hyp_num])\n",
    "            \n",
    "    assert(np.argmax(cur_probs) == len(cur_probs) - 1)\n",
    "            \n",
    "    return cur_hyps[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxF32HM9DuJ5"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation\n",
    "from importlib import reload\n",
    "\n",
    "evaluation = reload(evaluation)\n",
    "\n",
    "def validate(model, dataloader, w2i, i2w):\n",
    "    gts_dict = {}\n",
    "    hyps_dict = {}\n",
    "    for i, (image, texts) in enumerate(dataloader):\n",
    "        hyp = simple_beam_search(model, image, w2i, i2w, beam_size=3)[1:]\n",
    "        if hyp[-1] == w2i['<END>']:\n",
    "            hyp = hyp[:-1]\n",
    "        hyp = ' '.join([i2w[word] for word in hyp])\n",
    "        gts_dict[i] = texts[0]\n",
    "        hyps_dict[i] = [hyp]\n",
    "    return evaluation.compute_scores(gts_dict, hyps_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omJdXoxpDuJ9"
   },
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HRTgDdYDuJ-"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, dict_size, embedding_dim, hidden_size, *args, **kwargs):\n",
    "        super(SimpleModel, self).__init__(*args, **kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(in_features=13520, out_features=hidden_size)\n",
    "        self.encoder_layers = [\n",
    "            self.conv1, self.pooling, self.relu,\n",
    "            self.conv2, self.pooling, self.relu,\n",
    "            self.conv3, self.pooling, self.relu]\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=dict_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=dict_size)\n",
    "        \n",
    "    def encoder(self, image):\n",
    "        for layer in self.encoder_layers:\n",
    "            image = layer(image)\n",
    "        return self.linear1(image.view(-1, 13520)).view(-1, self.hidden_size)\n",
    "    \n",
    "    def decoder(self, image_vector, input_captions):\n",
    "        embeddings = nn.utils.rnn.PackedSequence(\n",
    "            self.embedding(input_captions.data),\n",
    "            input_captions.batch_sizes)\n",
    "        decoded, hiddens = self.rnn(embeddings, image_vector)\n",
    "        probs = self.linear2(decoded.data)\n",
    "        return nn.utils.rnn.PackedSequence(probs, decoded.batch_sizes), hiddens\n",
    "\n",
    "    def forward(self, image, input_captions):\n",
    "        image_vector = self.encoder(image)\n",
    "        image_vector = image_vector.unsqueeze(0)\n",
    "        return self.decoder(image_vector, input_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__zIFJewDuKD"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "\n",
    "class SimpleModelWithEncoder(nn.Module):\n",
    "    def __init__(self, dict_size, embedding_dim, hidden_size, *args, **kwargs):\n",
    "        super(SimpleModelWithEncoder, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # TODO: try to use mean instead of flatten all the features\n",
    "        self.linear1 = nn.Linear(in_features=100352, out_features=hidden_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=dict_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=dict_size)\n",
    "        \n",
    "    def encoder(self, image):\n",
    "        image = self.resnet(image)\n",
    "        return self.linear1(image.view(image.shape[0], -1)).view(-1, self.hidden_size)\n",
    "    \n",
    "    def decoder(self, image_vector, input_captions):\n",
    "        embeddings = nn.utils.rnn.PackedSequence(\n",
    "            self.embedding(input_captions.data),\n",
    "            input_captions.batch_sizes)\n",
    "        decoded, hiddens = self.rnn(embeddings, image_vector)\n",
    "        probs = self.linear2(decoded.data)\n",
    "        return nn.utils.rnn.PackedSequence(probs, decoded.batch_sizes), hiddens\n",
    "\n",
    "    def forward(self, image, input_captions):\n",
    "        image_vector = self.encoder(image)\n",
    "        image_vector = image_vector.unsqueeze(0)\n",
    "        return self.decoder(image_vector, input_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFIFra5gDuKG"
   },
   "outputs": [],
   "source": [
    "model = SimpleModelWithEncoder(dict_size=len(w2i), embedding_dim=512, hidden_size=512)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppRxKsgwDuKK"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3VPkawbDuKL"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad ], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABTV4ZwHDuKQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo7yYM1ZDuKV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    for image, inputs, outputs in trainloader:\n",
    "        image = image.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = outputs.to(device)\n",
    "\n",
    "        total_samples += image.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ans, _ = model(image, inputs)\n",
    "        loss = criterion(ans.data, outputs.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * image.shape[0]\n",
    "    total_loss /= total_samples\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(ROOT, 'model.pth'))\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "        scores = validate(model, valloader, w2i, i2w)\n",
    "\n",
    "    writer.add_scalar('loss', total_loss, epoch)\n",
    "    for score_name in scores:\n",
    "        writer.add_scalar(score_name, scores[score_name], epoch)\n",
    "    \n",
    "    print('-----')\n",
    "    print('Epoch: ', epoch)\n",
    "    print('Loss: ', total_loss)\n",
    "    print('Scores: ', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Hcdb_ovDuKZ"
   },
   "outputs": [],
   "source": [
    "# IN FEATURES NUM\n",
    "num_ftrs = model_ft.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arAfUfWzJBa2"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgtkyFhkJEfO"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModelWithEncoder(dict_size=len(w2i), embedding_dim=512, hidden_size=512)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Show and Tell.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
